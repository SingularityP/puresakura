【神经网络】基础的三大要素

最近正在学习计算智能，阅读教材为《人工神经网络理论、设计及应用》，所以计划做一系列跟此课程学习相关的笔记，巩固知识，加深交流。因为是学习笔记，难免会在知识的结构、内容上存在误解或疏漏，还请朋友们帮忙指正，欢迎交流。

#神经网络的三大要素

##1.1 建模
###1.1.1 生物上的神经网络
“人工神经网络”简称“神经网络”，是人类在人工智能领域上的一个重要尝试与探索。在阐述神经网络的三大要素之前，首先要建立相关的神经网络模型。人工神经网络，顾名思义，是人类对人脑结构模式和信息处理机制的模仿。

生物神经元的知识大家老早就学过了，我们就简单回顾一下。

如图 1.1 所示是一个典型的神经元，由树突、细胞体、轴突、突触四部分组成。来自其他细胞的化学信号（神经递质）与突触结合产生电信号，信号沿着树突传递，刺激神经元细胞产生兴奋或抑制，而后信号沿着轴突传递，到达轴突末端的突触并转换为化学信号，传递给下一个神经元。

另外，当刺激过小时，神经元并不会产生兴奋或者抑制，只有当刺激大过某一阈值时，神经元才会响应。而且当神经元处于兴奋或抑制状态时，无论来此其他神经元的刺激有多大，都不会再次做出响应，需要一定的时间恢复后才能再次接受刺激，这段时间成为不应期。

<div align="center">![物神经元示意图](/static/images/img_art/00155148928757954084b0485804f3dade2087270ef3c50000.jpg)
图1.1 生物神经元示意</div>

###1.1.2 神经网络的 6 点假设

根据上面的生物神经元信号的产生、处理与传递过程，我们可以在简化的基础上提出以下 **6 点假定**：

1. 多输入单输出
每个神经元都是一个多输入单输出的信息处理单元；

2. 两种输入
神经元输入分兴奋性输入和抑制性输入两种类型；

3. 两种性质
神经元具有空间整合特性和阈值特性；

4. 两种不变
神经元输入与输出见具有固定的时滞，主要取决于突触延搁；
神经元本身是非时变的，即其突触时延和突触强度均为常数；

5. 两种忽略
忽略时间整合作用和不应期。

###1.1.3 神经网络的图解模型

根据上述特点，可以建立如图1.2的神经元模型**图解表达**：

<div align="center">![神经元模型图解表达](/static/images/img_art/001551510283144c0581bc7704a4da4bc5a550231561f4a000.jpg)
图1.2 神经元模型图解表达</div>

其中$$x_1 \cdots  x_n$$ 为神经元的输入，$$o^j$$为神经元的输出，体现“多输入单输出”；

$$w_{ij}$$为神经元输入的权重，其正负体现了“两种输入”；

$$\Sigma$$表示神经网络对全部输入信号进行整合，而$$T$$为阈值，体现“两种性质”；

输入与输出的对应关系用$$f$$来表示，这种函数一般是非线性的。

建立的简单的图解模型，下面真正建立神经网络的数学模型，并完善提出的 6 点假设。

###1.1.4 神经网络的数学模型

令$$x_i(t)$$表示$$t$$时刻神经元$$j$$的输入，$$o_j(t)$$表示$$t$$时刻神经元$$j$$的输出，则有

$$\displaystyle o\_{j}(t)=f \left \\{ \left [ \sum\_{i=1}^n w\_{ij}x\_{i}(t-\tau\_{ij}) \right ] - T\_j \right \\}$$

式中，

$$\tau\_{ij}$$——输入输出间的突触时延；

$$T\_j$$——神经元$$j$$的阈值；

$$w\_{ij}$$——神经元$$i$$到$$j$$的突触强度，称为权重；

$$f(\cdot)$$——神经元转移函数。

下面对式子进行简化，令$$x\_{0}=-1$$，$$w\_{0j}=T$$，则

$$\displaystyle net\_j(t) = \sum\_{i=0}^n w\_{ij}x\_i(t) = W\_j^TX$$

因而有

$$o\_j = f(net\_j)=f(W\_j^TX)$$

这就是神经元的数学模型。

## 1.2 转移函数
神经元信息处理特性是神经网络三大要素之一，而神经元转移函数决定了神经元的信息处理特性。

最常用的转移函数有以下 4 种形式：

**（1）阈值型转移函数**

单极性阈值函数：

<div style="max-width:60%;margin:auto;">![单极性阈值函数](/static/images/img_art/001551513926489e13310ae73d9469e8615311dae5ce990000.jpg)</div>

> 这是神经元模型中最简单的一种，经典的 M-P 模型就属于这一类。

双极性阈值函数：

<div style="max-width:60%;margin:auto;">![双极性阈值函数](/static/images/img_art/0015515139367583a6f227ca6434931b419bcbd67cd5fb9000.jpg)</div>

> 这是神经元模型中常用的一种，处理离散信号问题时经常考虑用这种转移函数。

**（2）非线性转移函数**

单极性 Sigmoid 函数：

<div style="max-width:60%;margin:auto;">![单极性 Sigmoid 函数](/static/images/img_art/001551514391997f841f412fbfb430ebe983805dd3fab3e000.jpg)</div>

> 这是最长用的非线性转移函数，因其函数和导函数均是连续的，有非常好的光滑性，便于处理。

双极性 Sigmoid 函数：

<div style="max-width:60%;margin:auto;">![双极性 Sigmoid 函数](/static/images/img_art/0015515146319288708e63e8f5441018c240760bed5143c000.jpg)</div>

> 有时也采用这种双极性的。

**（3）分段线性转移函数**

<div style="max-width:60%;margin:auto;">![分段线性转移函数](/static/images/img_art/00155151509934912c17355cb4548b79bd13991c1ea45a4000.jpg)</div>

> 这种函数实现上比较简单，特点是神经元的输入和输出在一定区间内满足线性关系。

**（4）概率转移函数**

$$\displaystyle P\\{x=1\\}=\frac{1}{1+e^{-x/T}}$$

> 采用概率型转移函数时，神经元的输入和输出是不确定的。如上述热力学模型的例子，T 称为温度参数，使得神经元输出状态与热力学中的玻尔兹曼（Boltzmann）分布类似。

##1.3 拓补结构
神经网络的拓补结构是神经网络的三大要素之二，有多种分类方式。

根据神经元的**连接方式**，可以分为两大类：

**（1）层次型结构**

从神经元的功能层次来看，神经元可以分成若干层，如输入层、中间层（或隐藏层）和输出层，各层顺序相连。

再考虑到各层的连接情况，层次型网络结构就有 3 中典型的结合方式：

* 单纯层次型网络结构
* 输出层到输入层有连接的层次网络结构
* 层内有互连的层次网络结构。

三种情况的示意图如下图所示。

<div style="max-width:80%;margin:auto;">![层次型网络结构示意图](/static/images/img_art/00155152956127440d08276fdb4478c8864180c69d7f3b4000.jpg)</div>

**（2）互连型结构**

不同神经元相互之间的互连程度不同，从这个角度上看，有三种情况：

* 全互连型
* 局部互连型
* 稀疏连接型。

前两种情况的示意图如下图所示。

<div style="max-width:60%;margin:auto;">![互连型网络结构示意图](/static/images/img_art/0015515296668281958627ff72b46cf86ae22d98a44f677000.jpg)</div>

##1.4 学习规则
###1.4.1 通用学习规则
神经网络学习方式是神经网络的第三个要素。

人工神经网络的功能特性和智能体现由其连接的拓补结构和突触连接强度（即连接权值）决定。神经网络的全体连接权值可用一个矩阵$$W$$表示，其整体反映了神经网络对于所解决问题的知识存储。

而神经网络的学习或者说训练，其本质是可变权值的动态调整，其中，改变权值的规则称为**学习规则或学习算法**（训练规则或训练算法）。

神经网络的学习算法有很多，但可以总结为 3 类：

* 有监督学习：学习训练过程需要不断提供一个输入和期望的正确输出，根据实际输出与期望输出的差距动态调整权值。
* 无监督学习：学习过程需要不断提供输入，网络能够根据特有的内部结构和学习规则，发现任何可能存在的模式和规律。
* 灌输式学习：先将网络设计成能记忆特别的例子（一次性学习），以后当给定有关该例子的输入信息时，便被回忆起来。

以上三类学习算法，都可以用一种**通用学习规则**加以描述：权向量$$W\_{j}$$在$$t$$时刻的调整量$$\Delta W\_{j}(t)$$与$$t$$时刻的输入量$$X(t)$$和学习信号$$r$$的乘积成正比。其数学表达式为：

$$\Delta W\_j=\eta r[W\_j(t),X(t),d\_j(t)]X(t)$$

其中，$$\eta$$正的学习速率；$$d\_j$$为期望输出，又称标记，只在有监督学习时存在；$$r(\cdot )$$为学习规则，不同的学习规则有不同的含义。

###1.4.2 常用学习规则
下面对常用学习规则进行简要罗列，其具体应有将在后面的学习中展开。

![](/static/images/img_art/001551537931053b596028f031a4e72978dfd4544968631000.jpg)

**注释：**

(1) “突触修正”假设：当神经元的突触前膜电位于突触后膜电位同时为正时，突触传导增强；当突触前膜与突触后膜电位正负相反时，突触传导减弱。

(2) $$W\_{j^\ast}^T$$满足式$$\displaystyle W\_{j^{\ast}}^TX=max\_{i=1,2, \cdots ,p}(W\_i^TX)$$ 。

(3) 内星节点：总是接受来自个神经元的输入加权信号，是信号的汇聚点，对应的权重向量称为内星权向量；外星节点：总是向各个神经元发出加权信号，是信号的发散点，对应的权值向量称为外星权向量。