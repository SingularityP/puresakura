【神经网络】6 ART 网络

自适应共振理论，英文名 adaptive resonance theory，简称 ART，是仿真人类的心理认知活动建立的数学理论。目前基于 ART 已经有 ART I型网络，ART II 型网络和 ART III 型网络，功能越强，但也越复杂就是了。ART 网络最重要的特点就是较好地解决了新学的模式对已学习内容的“冲淡”问题。

## 5.3 ART 网络
自适应共振理论（adaptive resonance theory，ART）是 G.A.Carpenter 尝试建立人类心理和认知的数学理论的核心部分。目前发展出了 3 种形式：

1. ART I 型网络，处理双极型或二进制信号
2. ART II 型网络，是 ART I 型的扩展形式，用于处理连续型模拟信号
3. ART III 型网络，兼容前两种结构并扩大为任意层神经元网络

为什么会有 ART 网络呢？从之前的学习我们可以知道，神经网络学习的信息存储在权值矩阵中，但是矩阵所能记忆的知识是有限的，新输入的模式必然会对已经记忆的模式样本产生抵消或遗忘，从而是网络的分类性能受到影响。

这样 ART 网络就应运而生了，其较好地解决了稳定性和灵活性兼顾的问题。ART 网络的特点是非离线学习，即不是对输入集样本反复训练后才开始运行而是边学习边运行实时方式。

下面主要记录的是 ART I 型网络和 ART II 型网络。

### 5.3.1 ARTI 网络模型
ART 网络是**无监督**学习模型。

**(1) 拓补结构**

ART I 模型结构如图 5.1 所示，可以看出这种拓补结构与之前的非常不同，下面慢慢介绍。

ART I型网络有两层神经元构成两个子系统，分别称为比较层 C（注意子系统）和识别层 R（取向子系统），同时还有 3 中控制信号：复位信号（Reset），逻辑控制信号 G1 和 G2。

<div style="max-width:80%;margin:auto;text-align:center;">![](/static/images/img_art/001553654557743dd4bb42e8f9f42c382b89701e1b05b9b000.png)
图5.1 ART 网络拓补结构

</div>

* C 层结构

C 层具体结构如图 5.2(a) 所示，该层有$$n$$个节点，每个节点接收来自3个方面的信号：外界的输入信号$$x\_i$$，R 层的获胜神经元的外星向量$$T\_{j^\*}$$的返回信号$$t\_{ij^\*}$$和控制信号$$G1$$. C 层节点的输出是根据 $$2/3$$ 的“多数表决”产生的，即输出值$$c\_i$$与$$x\_i$$、$$t\_{ij^\*}$$、$$G\_1$$这 3 个信号中占多数的信号值相同。

* R 层结构

R 层具体结构如图 5.2(b) 所示，该层有$$m$$个节点，表示输入模式的维数。$$m$$可以动态增长，以设立新的模式类。C 层输出向量$$C$$沿$$m$$个内星权向量$$B\_j (j=1,2, \cdots, m)$$向前传送，达到 R 层各个神经元节点后经过竞争在产生获胜节点$$j^\*$$，指示本次输入模式所属类别，并输出 1，其余节点输出 0. 然后 R 层通过该输选择相应的外星权向量$$T\_{j^\*}$$。R 层有两个权值矩阵：内星权值矩阵和外星权值矩阵。

* 控制信号

G2 - 检测输入模式$$X$$是否为 0，等价于$$X$$各个分量的逻辑“或”。

G1 - 在网络开始时使 C 层直接输出信号 $$X$$，之后，使 C 层将 $$x\_i$$ 和 $$t\_{ij^\*}$$比较后输出，等价于$$G\_1=G\_2\bar{R}\_0$$，其中$$R\_0$$为R层输出向量各分量的逻辑“或”。

Reset - 基于某种事先设定的测量标准，如果$$T\_{j^\*}$$与$$X$$未达到预先设定的相似度$$\rho$$，则使 R 层竞争获胜神经元无效。

<div style="max-width:80%;margin:auto;text-align:center;">![](/static/images/img_art/001553654922154b9f1ae5fa8674e45888cd7054adcf3af000.png)
图5.2 比较层、识别层结构示意图

</div>

**(2) 转移函数与学习规则**


R 层使用了竞争学习机制，转移函数使获胜神经元输出 1，其余神经元输出 0，然后再通过外星权值矩阵生成典型向量输出到 C 层。然而其学习规则与竞争学习规则不同。

外星权向量的学习规则为：

```katex
t_{ij^*}(t+1) = t_{ij^*}(t) x_i \; \; i=1,2, \cdots, n; \; j^* \in J^* \; \; \; \; (a)
```

内星权向量的学习规则为：

```katex
\displaystyle
b_{ij^*}(t+1) = \frac{t_{ij^*}(t) x_i}{\displaystyle 0.5 + \sum_{i=1}^n t_{ij^*}(t) x_i} = \frac{t_{ij^*}(t+1)}{\displaystyle 0.5 + \sum_{i=1}^n t_{ij^*}(t+1)} \; \; \; \; (b)
```

其中，$$i \in \\{i \; | \; t\_{ij^\*} \neq 0 \\}$$，剩余的$$b\_{ij^\*}$$取初始值。

### 5.3.2 算法设计

#### 5.3.2.1 原理

ART 网络的运行可以分为四个阶段：

**(1) 匹配阶段**

**初始状态**。在未输入一个模式之前，网络处于等待状态，此时$$X=0$$，从而$$G2=0$$，R 层也输出零向量，使得$$\bar{R}\_0=1$$，最终$$G1 = G\_2\bar{R}\_0 = 0$$。

**输入状态**。然后输入一个模式$$X \neq 0$$，从而$$G2=1$$，此时由于输入数据未到 R 层，故 R 层仍输出零向量，使得$$\bar{R}\_0=1$$，最终$$G1 = G\_2\bar{R}\_0 = 1$$。

然后输入模式直接从 C 层输出，通过内星权值矩阵传至 R 层，其输入值作为匹配度：

```katex
\displaystyle
net_j = B_j^T X = \sum_{i=1}^n b_{ij}x_i \; \; \; \; j=1,2,\cdots,m
```

经过竞争选择具有最大匹配度的节点$$j^\*$$输出$$1$$，其他节点输出$$0$$，设输出向量为$$y$$。

**(2) 比较阶段**

R 层的输出向量将通过外星权值矩阵输出值 C 层，此时输出的向量称为典型向量：

```katex
\displaystyle
z_i = (Ty)_i = \sum_{j=1}^m t_{ij}y_j = t_{ij^*} \; \; \; \; i=1,2,\cdots,m
```

**比较状态**。此时，由于输入模式$$X \neq 0$$，从$$G2=1$$，R 层输出$$z \neq 0$$，故$$\bar{R}\_0=0$$，最终$$G1 = G\_2\bar{R}\_0 = 0$$，所以 C 层的最新输出状态 $$C \rq$$ 取决于 R 层返回的典型向量$$z$$和网络输入模式$$X$$的比较结果。

该比较结果$$C \rq$$反映了在匹配阶段 R 层竞争排名第一的模式类的典型向量$$z$$与当前输入模式的相似程度。下面通过两者定义相似程度$$N_0 / N_1$$：

```katex
\displaystyle
N_0 = X^Tz = \sum _{i=1} ^n x_it_{ij^*}, \; \; \; \; N_1 = \sum_{i=1}^n x_i
```

如果相似度$$N_0 / N_1$$小于警戒门限$$\rho \in (0,1)$$，则说明输入模式$$X$$与典型向量$$z$$相似程度不满足要求，网络发出 Reset 信号使得匹配阶段的竞争获胜阶段无效，网络进入下面的*搜索阶段*。

若果相似度$$N_0 / N_1$$大于警戒门限$$\rho \in (0,1)$$，则说明输入模式$$X$$与典型向量$$z$$相似程度满足要求，称$$X$$与$$z$$发生“共振”，网络进入下面的*学习阶段*。

**(3) 搜索阶段**

首先要重置（Reset）信号，在后续过程中持续抑制当前获胜阶段的输出，直到输入下一个新的模式为止。先进入匹配阶段，R 层受到抑制后，输出$$\bar{R}\_0=1$$，而输入模式$$X \neq 1$$，$$G2=1$$，最终$$G1 = G\_2\bar{R}\_0 = 1$$，这样就重新回到匹配阶段的输入状态，即允许输入模式直接从 C 层输出到 R 层。

不过重新进入匹配阶段后，之前竞争获胜神经元受到抑制，因此上次匹配程度次之的节点获胜，然后进入比较阶段，输出向量$$y$$，输出典型向量$$z$$，到达比较状态。

如果比较阶段的所有 R 层神经元的相似度都小于进阶门限，则说明当前输入模式无类可归，需要在 R 层增加一个节点来代表并存储该模式类，为此将其内星权向量$$B\_{j^\*}$$设置为当前输入模式向量，外星权向量$$T\_{j^\*}$$设置全$$1$$。

**(4) 学习阶段**

对发生“共振”的获胜节点对应的模式类要加强学习，先按照式 (a) 调整外星权向量，然后按照式 (b) 调整内星权向量，这样就能够使以后出现与该模式相似的输入样本时，获得更大的共振。

下面解释一下 ART 网络的两种记忆方式：

* C 层和 R 层输出信号称为短期记忆，用 STM（short time memory）表示
	* 对于 C 层，网络开始时其直接将输入模式 X 输出（即 y），故是对输入模式 X 的记忆
	* 而当 R 层信号 z 返回时，C 层失去原有记忆而输出 z 与 X 的比较信号
	* 另外，当重置信号作用于 R 层时，R 层输出向量也重置，失去记忆
* 两层之间的内星权值矩阵 B 和外星权值矩阵 T 称为长期记忆，用 LTM（long time memory）表示
	* 权值矩阵 B 和 T 只在学习阶段进一步加强记忆，而不管输入模式的改变

#### 5.3.2.2 算法
ART I 网络可以由软件实现，也可以由硬件实现。下面是软件实现方式：

> 一、匹配阶段
> (1) 初始化
> &nbsp; &nbsp; &nbsp; &nbsp; 权初始值对整个算法影响重大，应按照下面的方式设置权初值。对于内星权值矩阵$$B$$，要保证输入向量能够收敛到其应属类别而不轻易动用未使用节点；对于外星权值矩阵$$T$$，要保证对模式进行相似性测量时能正确计算其相似性。
> &nbsp; &nbsp; &nbsp; &nbsp; 对内星权值矩阵$$B$$赋予相同的较小数值，如：
```katex
\displaystyle
b_{ij}(0) = \frac{1}{1+n} \; \; i=1,2,\cdots,n; \; j=1,2,\cdots,m
```
> &nbsp; &nbsp; &nbsp; &nbsp; 对内星权值矩阵$$T$$各元素赋予值 1：
```katex
t_{ij} = 1 \; \; i=1,2,\cdots,n; \; j=1,2,\cdots,m
```

> (2) 输入模式
> &nbsp; &nbsp; &nbsp; &nbsp; 接收一个输入模式，$$X=(x\_1, x\_2, \cdots, x\_m), \; x\_i \in (0,1)^n$$。

> (3) 匹配度计算
> &nbsp; &nbsp; &nbsp; &nbsp; 使用内星权值矩阵计算 R 层各个节点的匹配度：
```katex
net = B^TX
```

> (4) 寻找最佳匹配节点
> &nbsp; &nbsp; &nbsp; &nbsp; 选择匹配值最大的节点作为最佳匹配点，输出向量$$y$$：
```katex
y_j = \begin{cases}
   0 & j \neq j^* \\
   1 & j = j^*
\end{cases}
```

> 二、比较阶段
> (5) 相似度计算
> &nbsp; &nbsp; &nbsp; &nbsp; R 层获胜节点输出典型向量：
```katex
z = Ty
```
> &nbsp; &nbsp; &nbsp; &nbsp; C 层将对$$z$$与$$X$$进行比较：
```katex
\displaystyle
N_0 = X^Tz = \sum _{i=1} ^n x_it_{ij^*}, \; \; \; \; N_1 = \sum_{i=1}^n x_i
```

> (6) 警戒门限检验
> &nbsp; &nbsp; &nbsp; &nbsp; 判断此时的相似度$$N\_0 / N\_1$$与警戒门限$$\rho$$的大小情况。
> &nbsp; &nbsp; &nbsp; &nbsp; 如果$$N\_0 / N\_1 < \rho$$，说明相似度不满足要求，则置$$y_{j^*}=0$$，然后先进入步骤 (7) 的搜索阶段。
> &nbsp; &nbsp; &nbsp; &nbsp; 若果$$N\_0 / N\_1 > \rho$$，说明输入模式应归为当前最佳匹配节点类，可以直接进入步骤(8) 的学习阶段。

> 三、搜索阶段
> (7) 搜索匹配模式类
> &nbsp; &nbsp; &nbsp; &nbsp; 抑制当前不满足条件的最佳匹配节点，然后转向步骤 (4)，重新寻找新的最佳匹配节点。
> &nbsp; &nbsp; &nbsp; &nbsp; 如果找不到最佳获胜节点，则说明当前输入模式无类可归，需要增加一个新的 R 层节点，使$$B\_{\text{new}}=X$$，$$t\_{i,\text{new}}=1，i=1,2, \cdots, n$$，然后再到下一步调整权值。

> 四、学习阶段
> (8) 调整网络权值
> &nbsp; &nbsp; &nbsp; &nbsp; 只有获胜神经元有权修改权值。首先修改获胜节点的外星权向量：
> &nbsp; &nbsp; &nbsp; &nbsp; 外星权向量的学习规则为：
```katex
t_{ij^*}(t+1) = t_{ij^*}(t) x_i \; \; i=1,2, \cdots, n; \; j^* \in J^* \; \; \; \; (a)
```

> &nbsp; &nbsp; &nbsp; &nbsp; 然后修改获胜节点的内星权向量，学习规则为：
```katex
\displaystyle
b_{ij^*}(t+1) = \frac{t_{ij^*}(t) x_i}{\displaystyle 0.5 + \sum_{i=1}^n t_{ij^*}(t) x_i} = \frac{t_{ij^*}(t+1)}{\displaystyle 0.5 + \sum_{i=1}^n t_{ij^*}(t+1)} \; \; \; \; (b)
```

> &nbsp; &nbsp; &nbsp; &nbsp; 其中，$$i \in \\{i \; | \; t\_{ij^\*} \neq 0 \\}$$，剩余的$$b\_{ij^\*}$$取初始值$$\displaystyle \frac{1}{1+n}$$。

#### 5.3.2.3 实现
有了上述算法，我们就可以用代码实现实现。

现在有四个模式：

<div style="max-width:100%;margin:auto;text-align:center;">![](/static/images/img_art/001553866004784f1b0f81a54cd4d0698bbd8ecf4188644000.png)
图5.3 四个输入模式

</div>

量化为以下输入向量：

```katex
X^A = (1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1)^T \\
X^B = (1,0,0,0,1,0,1,0,1,0,0,0,1,0,0,0,1,0,1,0,1,0,0,0,1)^T \\
X^C = (1,0,0,0,1,0,1,0,1,0,1,1,1,1,1,0,1,0,1,0,1,0,0,0,1)^T \\
X^D = (1,0,0,0,1,1,1,0,1,1,1,1,1,1,1,1,1,0,1,1,1,0,0,0,1)^T
```

现在用 Python 建立 ART I 型神经网络进行分类。先输入数据，这里数据从文件读入。

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

'''数据处理'''
X = pd.read_csv('ARTI_Data.txt', header=None).T
P = X.shape[1] # 模式个数
```

然后建立、配置网络：

```python
'''网络参数'''
n = X.shape[0] # 比较层 C 节点个数
m = 1 # 识别层 R 节点个数
B = np.ones((n,m))/(1+n) # 内星权值矩阵
T = np.ones((n,m)) # 外星权值矩阵
rho = 0.6 # 警戒门限
```

接下来就可以按照上述算法训练 ARTI 网络了：

```python
'''训练网络'''
for p in range(P):
    # 匹配阶段
    x = np.array(X.iloc[:,p]).reshape(n,1) # 输入模式
    net = B.T.dot(x) # 计算匹配度
    while True:
        index = np.argmax(net) # 寻找最佳匹配节点
        y = np.zeros((m,1)) # R 层输出向量
        y[index] = 1
        
        # 比较阶段
        y = T.dot(y) # R 层典型向量
        N0 = np.sum(y.T.dot(x))
        N1 = np.sum(x)
        similarity = N0/N1 # 计算相似度
        
        # 搜索阶段
        if net[index] == 0:
            m += 1
            index = m-1
            B = np.hstack((B,x.reshape(n,1)))
            T = np.hstack((T,np.ones((n,1))))
            break # 搜索阶段出口1
        if similarity < rho:
            net[index] = 0
        else:
            break # 搜索阶段出口2

    # 学习阶段
    T[:,index] = T[:,index]*x.reshape(n) # 调整外星权向量
    B[:,index] = T[:,index]/(0.5 + np.sum(T[:,index])) # 调整内星权向量
    print('样本 %d 属第 %d 类\n' % (p,index))
    B[(T[:,index] == 0),index] = 1/(1+n) # 其余为初始值
```

输出结果如下：

	样本 0 属第 0 类

	样本 1 属第 1 类

	样本 2 属第 1 类

	样本 3 属第 2 类