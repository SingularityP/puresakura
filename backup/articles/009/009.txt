【神经网络】5 SOM网络

自组织特征映射网，英文名 self-organizing feature map，简称 SOM，又称 Kohonen 网，是一种基于竞争学习规则、能够自我组织和学习的神经网络模型。这篇笔记介绍了 SOM 网络的三大要素，并设计了相应的学习算法，最后对 SOM 网络进行了简单的功能分析。

# 5 SOM 网络
## 5.1 基本概念
SOM 网络是**无监督**学习模型。

之前我们学习的**感知器**以及其重要改进**BP神经网络**，都属于有监督学习神经网络，反映了人脑**“自我反思”**的学习过程。同时，还应该注意到，人获得知识另一个重要途径是**“无师自通”**，即通过对客观事物的反复观察、分析与比较，自行揭示其内在规律，并对其共有特征的事物进行正确归类。

自组织特征映射神经网络就是基于这种目的诞生的，其属于层次型网络，有多种类型（后面会介绍重要的几种），其共同特点都是**具有竞争层**，通常使用【神经网络】1 节中提到的竞争学习规则或其改进规则。

下面**回顾并推导**关于竞争学习这一共有特点的详细内容。

**(1) 术语**

* 模式：对某些感兴趣的客体的定量描述或结构描述。
* 模式类：具有某些共同特征的模式的集合。
* 分类：在类别知识等期望输出的指导下，将待识别的输入模式分配到各自的模式类中。
* 聚类：无期望输出（无监督）下的分类，目的是将**相似**的模式样本划归一类，而将不相似的分离开。
* 相似性：衡量样本之间的相似程度，是聚类分析中要解决的重要问题，一般有下面的 3 种测量方式。
	* 欧式距离法 - 通过两个向量之间欧氏距离大小衡量相似程度，设定一个阈值距离作为聚类判据。倾向于形成大小相似且紧密的圆形聚类。
	* 余弦法 - 通过两个向量之间的夹角大小衡量相似程度，设定一个阈值角度作为聚类判据。倾向于形成大体同向的狭长型聚类。
	* 内积法 - 通过两个向量之间的内积值大小衡量相似度。
	
注 1：相似性的 3 种公式

欧氏距离：$$\displaystyle \lVert X - X\_i \rVert = \sqrt{( X - X\_i )^T( X - X\_i )}$$

余弦：$$\displaystyle \cos \psi = \frac{ X^T X }{ \lVert X \rVert \lVert X\_i \rVert }$$

内积：$$\displaystyle X^TX\_i = \lVert X \rVert \lVert X\_i \rVert \cos \psi$$

注 2：归一化公式

```katex
\displaystyle \hat{X} = \frac{X}{ \lVert X \rVert } = \left [ \frac{x_1}{ \sqrt{ \sum _{j=1}^{n} x_j^2 } } \cdots \frac{x_n}{ \sqrt{ \sum _{j=1}^{n} x_j^2 } } \right ]
```

**(2) 竞争学习过程**

竞争学习的思想来自生物学中人脑中的一个侧抑制现象：当一个神经元兴奋后，会对其周围的神经元细胞产生抑制作用。

下面详细叙述一下竞争学习规则，或者说胜者为王（Winner-Take-All）的流程。

1 向量归一化

将自组织网络中的输入模式向量$$X$$和竞争层中各神经元对应的内存权向量$$W\_j$$进行归一化，得$$\hat{X}$$和$$\hat{W\_j}(j=1,2,\cdots,m)$$。

2 寻找获胜神经元

使用相似性公式测量输入模式向量$$\hat{X}$$与个内星权向量$$\hat{W\_j}(j=1,2,\cdots,m)$$的相似性。以欧式距离为例：

```katex
\displaystyle \lVert \hat{X} - \hat{W}_{j^*} \rVert = \min _{j \in 1,2,\cdots,m} \left \{ \lVert \hat{X} - \hat{W}_{j} \rVert \right \}
```
```katex
\displaystyle \dArr \lVert \hat{X} - \hat{W}_{j^*} \rVert = \sqrt{2(1-\hat{W}_{j^*}^T \hat{X})}
```
```katex
\displaystyle \hat{W}_{j^*}^T \hat{X} = \max _{j \in 1,2,\cdots,m} ( \hat{W}_j^T \hat{X} )
```

3 网络输出与权值调整

胜者为王竞争学习规则规定，获胜神经元输出为 1，其余输出为 0：

```katex
o_j(t+1) = \begin{cases}
   1 & j = j^* \\
   0 & j \not = j^*
\end{cases}
```

只有获胜神经元才有权以学习率$$\alpha \in (0,1]$$调整其权向量$$W\_{j^*}$$，相当于侧抑制其他神经元的兴奋：

```katex
\begin{cases}
   W_{j^*}(t+1) = \hat{W}_{j^*}(t) + \Delta W _{j^*} = \hat{W}_{j^*}(t) + \alpha(\hat{X} - \hat{W}_{j^*}) & j = j^* \\
   W_j (t+1) = \hat{W_j}(t) & j \not = j^*
\end{cases}
```

**(3) 竞争学习原理**

通过上面的过程就可以实现自动分类，下面我们来直观地理解一下其中的道理，特别是权值调整公式的作用。

以 2 维数据为例，假设我们有大致可以分为 4 类的样本，归一化后将各个样本向量画在单位圆上，如图 5.1 (a) 所示的空心圆$$\circ$$。竞争层有 4 个神经元，其对应的 4 个内心权向量归一化后也标在同一单位圆上，如图 5.1 (a) 所示的实心圆$$\bullet$$。

<div style="max-width:60%;margin:auto;text-align:center;">![](/static/images/img_art/001553153884031d8048987affb4499ac259b9037477499000.png)
图5.1 竞争学习原理

</div>

根据欧氏距离相似性（其他相似性定义同理）的定义，权值向量与样本向量越靠近，两者的内积也就越大，从而离样本向量最近的权值向量获胜。

然后，进行权值调整时，从向量角度看，上一步的权向量加上权值向量与样本向量之差，如图 5.1 (b) 所示，权值向量向样本向量靠拢。

这样，单位元上的 4 个$$\bullet$$点会逐渐移入各输入模式的簇中心，从而使竞争的每个神经元的权向量成为输入模式的一个聚类中心。

## 5.2 SOM 网络

基于竞争学习规则，可以建立自组织特征映射神经网络（self-organizing feature map，简称 SOM），又称为 Kohonen 网。如果说竞争学习借鉴了生物学中的“侧抑制”现象，那么 SOM 在此基础上借鉴了“人脑对特定外界刺激会在特定区域产生兴奋”这一现象。

### 5.2.1 网络模型

SOM 网络共有两层，输入层与 BP 网络形式相同，节点数与样本维数相等。输出层也是竞争层，有多种维度类型，图 5.2 是一维和二维的输出阵列**拓补结构**。

1. 一维线阵 - 在一条直线上，相邻神经元侧向连接
2. 二维平面阵 - 在一个平面上，每个神经元与其周围的神经元侧向连接
3. 三维栅格阵 - 在一个空间内，每个神经元与其四周的神经元侧向连接

<div style="max-width:60%;margin:auto;text-align:center;">![](/static/images/img_art/001553155598525f2c70030bf534482b5e1f7db18efce5b000.png)
图5.2 SOM 的输出阵列

</div>

SOM 网络的**学习规则**和**转移函数**继承了胜者为王学习规则及其规定的转移函数，即只有竞争获胜的神经元才有权调整权向量，同时，SOM 网络还进行了改进，允许获胜的神经元及其邻域内的神经元，按照某种递减函数进行权向量调整，该区域称为**权值调整域**。竞争层输出也不局限于非 0 即 1，其只是将样本信息按照相似性分类号反映到竞争层的线阵中，可以说输出的就是竞争层权值矩阵，其对样本的兴奋区域指出了样本的所属类别。**（关于输出内容的理解有待完善）**

### 5.2.2 算法设计

**(1) 原理**

类似竞争学习原理，SOM 网络也是在训练中，不断对获胜节点及其领域内的节点权向量进行调整，使其向输入模式向量的方向靠拢，调整力度由远及近、由大及小，从而也保证了收敛，避免来回波动。

最后，输出层各节点成为对特定模式类敏感的神经细胞，对应的内星权向量成为各输入模式类的中心权向量。对于未曾见过的输入模式，其也会将其归入最接近的模式类别。

来看一下维基百科上的过程示意图 5.3，蓝色为数据分布区域，白点为当前输入模式，黄点为获胜节点，其在权值调整过程中不断向输入模式靠近。

<div style="max-width:60%;margin:auto;text-align:center;">![](/static/images/img_art/001553159216763b25391b073e6425e9e95a939c35d7c43000.png)
图5.3 SOM 学习过程

</div>

**(2) 算法**

> (1) 数据初始化、归一化
> &nbsp; &nbsp; &nbsp; &nbsp; 对所有的输入模式进行归一化，得到$$\hat{X}^p, \; \; p \in \\{ 1,2,\cdots,P \\}$$，对竞争层的权值向量赋值小随机数，并进行归一化，得$$\hat{W}\_j, \; \; j \in \\{ 1,2,\cdots,m \\}$$；建立优胜邻域$$N\_{j^*}(t)$$和学习率$$\eta(t,N)$$（具体函数形式在后面的设计要点中详细说明）。

> (2) 输入模式
> &nbsp; &nbsp; &nbsp; &nbsp; 从训练集中**随机**选择一个输入模式。

> (3) 寻找获胜节点
> &nbsp; &nbsp; &nbsp; &nbsp; 根据公式
```katex
\displaystyle \hat{W}_{j^*}^T \hat{X} = \max _{j \in 1,2,\cdots,m} ( \hat{W}_j^T \hat{X} )
```
> &nbsp; &nbsp; &nbsp; &nbsp; 选出点积最大的获胜节点$$j^*$$；如果输入模式未归一化，应该选取欧氏距离最小的获胜节点。

> (4) 确定优胜邻域
> &nbsp; &nbsp; &nbsp; &nbsp; 以$$j^*$$为中心确定$$t$$时刻的权值调整域（时间形状、退火函数）。

> (5) 调整权值
> &nbsp; &nbsp; &nbsp; &nbsp; 对于优胜邻域$$N\_{j^*}(t)$$内的所有节点调整权值
```katex
w_{ij}(t+1) = w_{ij}(t) + \eta(t,N)[x_i^p - w_{ij}(t)] \; \; i = 1,2,\cdots,n; \; j \in N_{j^*}(t)
```

> (6) 重复步骤 (2) 至 (5)，直至学习率$$\eta (t)$$ 衰减到零或某个预定的正小数，或者到达最大迭代次数。

以上算法称为 Kohonen 算法。

**(3) 设计要点**

* 输出层设计

对于输出层的节点数问题，过少将导致样本“粗分”，过多将导致分得过细或出现“死节点”。因此，先设置较多的输出节点，以便较好地映射样本的拓补结构，如果分类过细，在酌情减少输出节点。若有死节点，一般可以通过重新初始化权值解决。

对于输出层节点排列问题，应该使排列形式尽量直观地反映出实际问题的物理意义。

* 权值初始化

一般情况下，SOM 网络的权值初始化为较小的随机数。

某些情况下，样本整体可能相对集中于高维空间的某个局部区域，导致训练结果是全部样本分为一类。解决思路是尽量使权值的初始位置与输入样本的大致分布区域充分重合。

方法一是从训练集中随机抽出$$m$$个输入样本作为初始权值：

```katex
W_j(0) = X^{k_{ram}} \; \; j = 1,2, \cdots, m \; \; k_{ram} \in \{ 1,2, \cdots, P \}
```

方法二是计算全体样本的中心向量，并在此基础上叠加小随机数：

```katex
\displaystyle
\bar{X} = \frac{1}{P} \sum_{p=1}^P X^p
\\
W_j(0) = \bar{X} + ram \; \; j = 1,2, \cdots, m 
```

* 优胜邻域

优胜邻域可以为正方形、六边形或圆形，如图 5.4 所示，其区域随着时间的增加而收缩。

<div style="max-width:60%;margin:auto;text-align:center;">![](/static/images/img_art/001553160940776e5b0aea4266141a2912647d48471874c000.png)
图5.4 优胜邻域

</div>

优胜邻域的大小用邻域半径$$r(t)$$表示，凭经验设计，如下，其中$$C_1$$为与输出层节点数$$m$$有关的正常数，$$B_1$$为大于$$1$$的常数，$$t_m$$为预先选定的最大训练次数。

```katex
\displaystyle
r(t) = C_1 \left ( 1- \frac{t}{t_m} \right ),
\; \; \; \;
r(t) = C_1 \exp \left ( \frac{-B_1 t}{t_m} \right )
```

* 学习率设计

学习率$$\eta(t,N)$$是训练时间$$t$$和邻域内第$$j$$个神经元与获胜神经元$$j^*$$之间拓补距离$$N$$的函数，其随着两者的增加而减少，例如：

$$\eta(t,N) = \eta(t) e^{-N}$$

其中$$\eta (t)$$是随时间单调下降的函数，称为退火函数。例如以下两种，其中$$C_2$$为$$0 \backsim 1$$之间的常数，$$B\_2$$为大于$$1$$的常数。

```katex
\displaystyle
\eta (t) = C_2 \left ( 1- \frac{t}{t_m} \right )
```

```katex
\displaystyle
\eta (t) = C_2 \exp \left ( \frac{-B_2 t}{t_m} \right )
```

### 5.2.3 算法实现
下面用 Python 试着实现上述算法，这里的目标是对西瓜种类进行分类（数据来自西瓜书）。

首先当然是导包啦：

```python
'''导包'''
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
```

然后就是准备一些数据并进行归一化处理：

```python
'''数据准备'''
# 输入模式（来自西瓜书）
data = np.array([
    [0.697,0.46],  [0.774,0.376], [0.634,0.264], [0.608,0.318], [0.556,0.215],
    [0.403,0.237], [0.481,0.149], [0.437,0.211], [0.666,0.091], [0.243,0.267],
    [0.245,0.057], [0.343,0.099], [0.639,0.161], [0.657,0.198], [0.36,0.37],
    [0.593,0.042], [0.719,0.103], [0.359,0.188], [0.339,0.241], [0.282,0.257],
    [0.748,0.232], [0.714,0.346], [0.483,0.312], [0.478,0.437], [0.525,0.369],
    [0.751,0.489], [0.532,0.472], [0.473,0.376], [0.725,0.445], [0.446,0.459]
])
P = X.shape[0] # 模式个数
n = X.shape[1] # 模式维度

# 竞争层权值矩阵（竞争层为 Wx x Wy 平面）
Wx = 2
Wy = 2
m = Wx*Wy # 竞争层节点个数
W = np.random.random([2,Wx*Wy])/10

# 最大迭代次数
tm = 300

# 优胜邻域
C1 = np.sqrt(m)/2 # 最大邻域半径
def neibors(winner, t):
    x,y = [winner//Wx, winner%Wx] # 优胜节点坐标
    partner = [] # 邻域节点
    r = int(C1*(1-t/tm)) # 邻域半径
    for i in range(2*r+1):
        for j in range(2*r+1):
            x_ = i+x-r
            y_ = j+y-r
            ind = x_*Wx + y_
            dist = np.sqrt((x-x_)**2+(y-y_)**2)
            #print('base: ', x, y, 'winner: ', winner, 'r: ', r)
            #print('the adjacent cell: (', x_, y_, ') index:', ind, 'dist:', dist)
            if x_ < 0 or y_ < 0 or x_>= Wx or y_ >= Wy or dist > r:
                continue
            partner.append((ind, dist))
    return partner

# 学习率
C2 = 0.3 # 初始学习率
def eta(dist, t):
    return C2*(1-t/tm)*np.exp(-dist)
    #return C2/(t+1)*np.exp(-dist)

# 数据归一化
X = data / np.sqrt(np.diag(data.dot(data.T)).reshape(P,1)).dot(np.ones([1,n])) # 输入模式
W = W / np.ones([n,1]).dot(np.sqrt(np.diag(W.T.dot(W)).reshape(1,m))) # 竞争层权值矩阵
```

准备就绪就可以训练了：

```python
'''训练'''
t = 0
while t < tm:
    print('iteration %d: ' % t)
    # 随机获取一个样本
    x = X[np.random.randint(0,P)].reshape(n,1)
    # 寻找获胜节点
    ind = np.argmax(W.T.dot(x))
    # 确定优胜邻域
    neibor = neibors(ind, t)
    print('Winner adjacent area is:')
    print(neibor)
    # 调整权值
    for ind,dist in neibor:
        w = W[:,ind].reshape(n,1)
        W[:,ind] = (w + eta(dist,t)*(x-w)).reshape(n)
    # 控制循环
    t += 1
```

简单测试一下，可以得到 5 个类别：

```python
'''测试'''
def test(X):
    # 结果字典
    result = {}
    # 数据测试
    for k in range(P):
        # 获取一个样本
        x = X[k].reshape(n,1)
        # 寻找获胜节点
        ind = np.argmax(W.T.dot(x))
        # 对结果进行归类
        if ind not in result:
            result[ind] = []
        result[ind].append(k)
    return result

# 测试
result = test(X)
result
```

	{6: [0, 5, 18, 22, 24, 25, 27, 28],
	 1: [1, 2, 3, 4, 7, 17, 21],
	 2: [6, 10, 11, 12, 13, 20],
	 0: [8, 15, 16],
	 8: [9, 14, 19, 23, 26, 29]}
	 
由于数据是二维的，可以进行可视化，绘图结果在图 5.5。

```python
'''绘图'''
def draw(result, dataset):
    for key in result.keys():
        a = []
        b = []
        datas = result[key]
        for data in datas:
            a.append(dataset[data][0])
            b.append(dataset[data][1])
        plt.scatter(a, b, marker='o', label=key)
    plt.legend(loc='upper right')
    plt.show()
    
# 归一化的数据图
draw(result, X)
# 原始的数据图
draw(result, data)
```

<div align="center" style="background-color: #999;width: max-content;margin: auto;margin-bottom: 1rem;">![](/static/images/img_art/0015536530680373c91467d09ce44b49330f689106821f9000.png) ![](/static/images/img_art/0015536530777726b7ee1bb4ee445df9aab306960d9f4dc000.png)
图5.5 归一化数据和原始数据分布图

</div>

### 5.2.4 功能分析

(1) 功能与特点

* 保序映射 - 将输入空间的样本模式类有序地映射到输出层上
* 数据压缩 - 将高维空间的样本保持在拓补空间不变的条件下投影到低维空间
* 特征提取 - 高维空间项低维空间映射，相当于提取特征

(2) 局限性

* 隐层神经元数目难以确定 - 神经元未充分利用，甚至成为“死节点”
* 认为确定学习速率、终止学习
* 隐层聚类结果与初始权值有关